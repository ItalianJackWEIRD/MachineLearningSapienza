ğŸ“ DQN Parameter Modification Report â€“ FrozenLake 8x8
ğŸ¯ Objective
Adapt the DQN agent to perform effectively on the more complex 8x8 FrozenLake-v1 environment, which has:

More states (64 vs. 16 in 4x4)

Longer paths to reach the goal

Higher likelihood of getting stuck or failing without sufficient exploration and training time.

ğŸ”§ Modifications Overview
Parameter	Old Value	New Value	Justification
Map Size	4x4	8x8	Larger environment, more complex task
Episodes	10,000	20,000	More episodes to allow the agent to explore and learn optimal paths
Hidden Layers	1 layer (256)	2 layers (128, 64)	Deeper network helps learn more complex patterns in larger state spaces
Gamma (Î³)	0.9	0.95	Longer horizons in 8x8 require emphasizing future rewards more
Epsilon Decay	linear	multiplicative (0.995)	Slower decay allows more exploration early in training
Minimum Epsilon	not set	0.01	Ensures continued exploration throughout training
Replay Memory	1000	80,000	Larger memory to capture more diverse experiences over longer episodes
Mini-Batch Size	32	256	Bigger batch helps stabilize training with larger memory
Target Sync Rate	1000	1500	Less frequent updates in larger env reduces instability in learning

ğŸ“ˆ Expected Impact
Improved Learning Stability: Larger memory and batch size reduce variance in updates.

Better Exploration: Gradual epsilon decay and longer training help avoid premature convergence.

More Expressive Model: Two hidden layers allow the agent to represent more nuanced Q-values in a 64-state environment.

ğŸ“ DQN Parameter Optimization Report â€“ FrozenLake 4x4 (Simplified Setup)
ğŸ¯ Objective
Optimize the DQN architecture and training hyperparameters for the FrozenLake 4x4 environment, which:

Has a small state space (16 states)

Is less complex compared to 8x8

Requires a lighter model and fewer training episodes

ğŸ”§ Modifications Summary
Parameter	Old Value (8x8)	New Value (4x4)	Justification
Map Size	8x8	4x4	Environment reduced to fewer states (16 total)
Episodes	20,000	5,000	Smaller env learns faster, fewer episodes suffice
Hidden Layers	2 (128, 64)	1 (64 nodes)	Less complexity needed for simpler environment
Gamma (Î³)	0.95	0.9	Lower discount factor as episodes are shorter
Epsilon Decay	multiplicative 0.995	linear or 0.99	Simpler tasks can tolerate faster decay
Minimum Epsilon	0.01	0.05	More greedy behavior allowed in smaller state space
Replay Memory	80,000	5,000	Less experience needed for learning
Mini-Batch Size	256	64	Smaller batch still provides stable updates
Target Sync Rate	1500	100	Faster updates improve learning in small environments

ğŸ§  Neural Network Structure
Component	8x8 Version	New 4x4 Version	Reason
Input Layer	64 inputs (one-hot)	16 inputs (one-hot)	One-hot state encoding per tile
Hidden Layers	2 layers (128 â†’ 64)	1 layer (64)	Simpler function approximation needed
Output Layer	4 actions (Q-values)	4 actions (Q-values)	Same action space in both environments

ğŸ“ˆ Expected Benefits
Faster Training: Simpler model, fewer episodes, smaller memory.

Good Convergence: One layer is sufficient to capture value function over 16 states.

Resource Efficient: Less memory and compute neededâ€”ideal for testing, debugging, and learning fundamentals.